{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <a id='1-1'> STA220 - Exploratory Yelp Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group: Anuraag Velamati, Yash Vekaria and Shubhankar Garg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T23:21:57.603845Z",
     "start_time": "2024-03-17T23:21:37.708685Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "from plotnine import *\n",
    "from wordcloud import WordCloud \n",
    "from pyecharts.charts import Pie\n",
    "from pyecharts import options as opts\n",
    "import plotly.express as px\n",
    "sys.path.insert(0, '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages')\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T23:28:22.250941Z",
     "start_time": "2024-03-17T23:28:21.790957Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../Other_codes/')\n",
    "sys.path.insert(0, '/usr/local/mysql/bin')\n",
    "from db_utils import YelpDb\n",
    "from Feature_Engineering import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section II: Build Web Crawler <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of best big cities in California are: ['San Diego ', 'San Francisco ', 'San Jose ', 'Sacramento ', 'Riverside ', 'Los Angeles ', 'Long Beach ', 'Anaheim ', 'Bakersfield ', 'Oakland ', 'Santa Ana ', 'Fresno ']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "url = 'https://patch.com/california/lajolla/ca-cities-rank-among-best-big-cities-live'\n",
    "response = requests.get(url)\n",
    "page_content = response.content\n",
    "soup = BeautifulSoup(page_content, 'html.parser')\n",
    "second_ol = soup.select_one('.page__content ol:nth-of-type(2)')\n",
    "cities_cleaned = [re.sub(\"[^A-Za-z ]\", \"\", li.text) for li in second_ol.find_all('li')]\n",
    "cities_to_display = cities_cleaned[:12]\n",
    "print(f'The list of best big cities in California are: {cities_to_display}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T23:28:29.957864Z",
     "start_time": "2024-03-17T23:28:29.047824Z"
    }
   },
   "outputs": [],
   "source": [
    "# from urllib.request import urlopen\n",
    "# link = 'https://patch.com/california/lajolla/ca-cities-rank-among-best-big-cities-live'\n",
    "# html = urlopen(link).read()\n",
    "# soup = BeautifulSoup(html, features='html')\n",
    "# target_level = soup.find('section', {\"class\": \"page__content\"})\n",
    "# target_level_2 = target_level.find_all('ol')[1]\n",
    "# target_level_3 = target_level_2.find_all('li')\n",
    "# cali_cities = []\n",
    "# token_list = []\n",
    "# for info in target_level_3:\n",
    "#     cali_cities.append(info.get_text(strip=True))\n",
    "# for i in cali_cities:\n",
    "#     flag = re.findall(\"[A-Za-z ]\", i)\n",
    "#     if flag:\n",
    "#         token = \"\".join(flag)\n",
    "#         token_list.append(token)\n",
    "# print(f'The list of best big cities in California are: {token_list[:12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of top 12 cities with most populations in California are: ['Los Angeles', 'San Diego', 'San Jose', 'San Francisco', 'Fresno', 'Sacramento', 'Long Beach', 'Oakland', 'Bakersfield', 'Anaheim', 'Stockton', 'Riverside']\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_largest_California_cities_by_population'\n",
    "html_content = urlopen(url).read()\n",
    "soup_parser = BeautifulSoup(html_content, 'html.parser')\n",
    "data_table = soup_parser.find('table', class_=\"wikitable sortable\")\n",
    "cities_list = [row.find_all('td')[1].text.strip() for row in data_table.find_all('tr')[1:]]\n",
    "print(f'The list of top 12 cities with most populations in California are: {cities_list[:12]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T23:28:33.399760Z",
     "start_time": "2024-03-17T23:28:33.027986Z"
    }
   },
   "outputs": [],
   "source": [
    "# link = 'https://en.wikipedia.org/wiki/List_of_largest_California_cities_by_population'\n",
    "# html = urlopen(link).read()\n",
    "# soup = BeautifulSoup(html, features=\"html\")\n",
    "# cities = []\n",
    "# table = soup.find('table', {\"class\": \"wikitable sortable\"})\n",
    "# rows = table.find_all(\"tr\")\n",
    "# for row in rows[1:]:\n",
    "#     city = row.find_all(\"td\")[1].get_text(strip=True)\n",
    "#     cities.append(city)\n",
    "# print(f'The list of top 12 cities with most populations in California are: {cities[:12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T23:28:53.609896Z",
     "start_time": "2024-03-17T23:28:53.490956Z"
    }
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(2003, \"Can't connect to MySQL server on 'localhost' ([Errno 61] Connection refused)\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pymysql/connections.py:644\u001b[0m, in \u001b[0;36mConnection.connect\u001b[0;34m(self, sock)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/socket.py:844\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 844\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/socket.py:832\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    831\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m--> 832\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Data_In_DataBase \u001b[38;5;241m=\u001b[39m \u001b[43mYelpDb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myelp_db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m table_showing \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow TABLES;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m Data_In_DataBase\u001b[38;5;241m.\u001b[39mexecute(table_showing)\n",
      "File \u001b[0;32m~/Documents/PhD Course Work/STA 220 Data & Web Technologies for Data Science/Project/YelpTest/Final Project/STA220/IPYNB_files/../Other_codes/db_utils.py:16\u001b[0m, in \u001b[0;36mYelpDb.__init__\u001b[0;34m(self, db_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb \u001b[38;5;241m=\u001b[39m db_name\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn \u001b[38;5;241m=\u001b[39m \u001b[43mpymysql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                            \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcharset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mcursor()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pymysql/connections.py:358\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, user, password, host, database, unix_socket, port, charset, collation, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, read_default_group, autocommit, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout, bind_address, binary_prefix, program_name, server_public_key, ssl, ssl_ca, ssl_cert, ssl_disabled, ssl_key, ssl_verify_cert, ssl_verify_identity, compress, named_pipe, passwd, db)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/pymysql/connections.py:711\u001b[0m, in \u001b[0;36mConnection.connect\u001b[0;34m(self, sock)\u001b[0m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mprint\u001b[39m(exc\u001b[38;5;241m.\u001b[39mtraceback)\n\u001b[0;32m--> 711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If e is neither DatabaseError or IOError, It's a bug.\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# But raising AssertionError hides original error.\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# So just reraise it.\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mOperationalError\u001b[0m: (2003, \"Can't connect to MySQL server on 'localhost' ([Errno 61] Connection refused)\")"
     ]
    }
   ],
   "source": [
    "Data_In_DataBase = YelpDb(\"yelp_db\")\n",
    "table_showing = \"show TABLES;\"\n",
    "Data_In_DataBase.execute(table_showing)\n",
    "Data_In_DataBase.fetch_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Read all crawled databases tables into DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T03:46:03.140879Z",
     "start_time": "2024-02-16T03:46:02.715199Z"
    }
   },
   "outputs": [],
   "source": [
    "Sacramento_data = Data_In_DataBase.df_conversion(\"Sacramento_Table\")\n",
    "Oakland_data = Data_In_DataBase.df_conversion(\"Oakland_Table\")\n",
    "Riverside_data = Data_In_DataBase.df_conversion(\"Riverside_Table\")\n",
    "San_Diego_data = Data_In_DataBase.df_conversion(\"San_Diego_Table\")\n",
    "LA_data = Data_In_DataBase.df_conversion(\"Los_Angeles_Table\")\n",
    "Fresno_data = Data_In_DataBase.df_conversion(\"Fresno_Table\")\n",
    "Bakersfield_data = Data_In_DataBase.df_conversion(\"Bakersfield_Table\")\n",
    "Anaheim_data = Data_In_DataBase.df_conversion(\"Anaheim_Table\")\n",
    "Long_Beach_data = Data_In_DataBase.df_conversion(\"Long_Beach_Table\")\n",
    "Santa_Ana_data = Data_In_DataBase.df_conversion(\"Santa_Ana_Table\")\n",
    "San_Jose_data = Data_In_DataBase.df_conversion(\"San_Jose_Table\")\n",
    "San_Fransisco_data = Data_In_DataBase.df_conversion(\"San_Francisco_Table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T04:01:15.563209Z",
     "start_time": "2024-02-16T04:01:14.823408Z"
    }
   },
   "outputs": [],
   "source": [
    "Data_of_Cities = cities_list[:12]\n",
    "DataFrame_data = [LA_data.shape[0], San_Diego_data.shape[0], San_Jose_data.shape[0], San_Fransisco_data.shape[0],\n",
    "               Fresno_data.shape[0], Sacramento_data.shape[0], Long_Beach_data.shape[0], Oakland_data.shape[0],\n",
    "               Bakersfield_data.shape[0], Anaheim_data.shape[0], Santa_Ana_data.shape[0], Riverside_data.shape[0]]\n",
    "comp_dict = {\"Cities\": Data_of_Cities, \"Restaurants_number\": DataFrame_data}\n",
    "comp_df = pd.DataFrame(data=comp_dict).sort_values(\"Restaurants_number\", ascending=False)\n",
    "plt.figure(figsize=(14, 7))\n",
    "bar_plot = sns.barplot(data=comp_df, y='Cities', x='Restaurants_number', alpha=0.8)\n",
    "bar_plot.set_title(\"Crawled number of restaurants per city\", fontsize=20)\n",
    "bar_plot.set_xlabel(\"Number of crawled restaurants\", fontsize=15)\n",
    "bar_plot.set_ylabel(\"Crawled cities\", fontsize=15)\n",
    "for bar in bar_plot.patches:\n",
    "    bar_plot.annotate(format(bar.get_width(), '.0f'), \n",
    "                      (bar.get_width(), bar.get_y() + bar.get_height() / 2), \n",
    "                      ha='center', va='center',\n",
    "                      size=15, xytext=(5, 0), \n",
    "                      textcoords='offset points')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T04:02:51.271898Z",
     "start_time": "2024-02-16T04:02:51.178656Z"
    }
   },
   "outputs": [],
   "source": [
    "Combined_Dataframe = pd.concat([LA_data, San_Diego_data, San_Jose_data, San_Fransisco_data,\n",
    "                        Fresno_data, Sacramento_data, Long_Beach_data, Oakland_data,\n",
    "                        Bakersfield_data, Anaheim_data, Santa_Ana_data, Riverside_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T04:03:07.135587Z",
     "start_time": "2024-02-16T04:03:07.105664Z"
    }
   },
   "outputs": [],
   "source": [
    "Combined_Dataframe = Combined_Dataframe.drop_duplicates()\n",
    "Combined_Dataframe = Combined_Dataframe.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T04:03:14.857644Z",
     "start_time": "2024-02-16T04:03:14.760303Z"
    }
   },
   "outputs": [],
   "source": [
    "# fill NAN\n",
    "exceptions(Combined_Dataframe, verbose=True)\n",
    "Final_DataFrame_Combined = Combined_Dataframe.applymap(find_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_DataFrame_Combined=Final_DataFrame_Combined.drop(0)\n",
    "Final_DataFrame_Combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpeningTime(time_str):\n",
    "    if time_str == None:\n",
    "        return None\n",
    "    if time_str == 'Closed':\n",
    "         return 'Closed'\n",
    "    print(\"Actual string\", time_str)\n",
    "    components = str(time_str[0][0]).split()\n",
    "    print(\"time_str:\",components)\n",
    "    print(\"Opening component:\",type(components[0]))\n",
    "    opening_hour_component = str(components[0]).split(':')\n",
    "    if len(opening_hour_component) != 2:\n",
    "        return \"Closed\"\n",
    "    opening_hour, opening_minute = map(int, opening_hour_component)\n",
    "    if 'PM' in components[1] and opening_hour != 12:\n",
    "        opening_hour += 12\n",
    "    opening_minute = opening_minute if opening_minute != 0 else '00'\n",
    "    return str(opening_hour)+\":\"+str(opening_minute)\n",
    "time_str = [['3:00 PM - 9:30 PM']]\n",
    "hour_24_format = OpeningTime(time_str)\n",
    "print(hour_24_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClosingTime(time_str):\n",
    "    if time_str == None or time_str==\"Unknown\":\n",
    "        return 'No Info'\n",
    "    if time_str == 'Closed':\n",
    "        return 'Closed'\n",
    "    try:\n",
    "        print(\"Actual string\", time_str)\n",
    "        components = str(time_str[0][1]).split()\n",
    "        print(\"time_str:\",components)\n",
    "        print(\"closing component:\",components[0])\n",
    "    except:\n",
    "        components = str(time_str).split(\" - \")[-1].split()[0:2]\n",
    "        print(time_str, \"********************\", components)\n",
    "    closing_hour_component = str(components[0]).split(':')\n",
    "    if len(closing_hour_component) != 2:\n",
    "        return \"Closed\"\n",
    "    closing_hour, closing_minute = map(int, closing_hour_component)\n",
    "    if 'PM' in components[1] and closing_hour != 12:\n",
    "        closing_hour += 12\n",
    "    closing_minute = closing_minute if closing_minute != 0 else '00'\n",
    "    return str(closing_hour)+\":\"+str(closing_minute)\n",
    "time_str = [['9:00 PM ','10:30 PM']]\n",
    "time_str = [['7:00 AM ','11:00 AM']]\n",
    "closing_hour_24 = ClosingTime(time_str)\n",
    "print(closing_hour_24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_DataFrame_Combined[['MonOpen', 'TueOpen', 'WedOpen', 'ThuOpen', 'FriOpen', 'SatOpen', \n",
    "               'SunOpen']] = Final_DataFrame_Combined[['Mon', 'Tue', 'Wed', 'Thu', 'Fri', \n",
    "                                            'Sat', 'Sun']].applymap(OpeningTime)\n",
    "\n",
    "Final_DataFrame_Combined[['MonClose', 'TueClose', 'WedClose', 'ThuClose', 'FriClose', 'SatClose', \n",
    "               'SunClose']] = Final_DataFrame_Combined[['Mon', 'Tue', 'Wed', 'Thu', 'Fri', \n",
    "                                            'Sat', 'Sun']].applymap(ClosingTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_DataFrame_Combined[\"Fri\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T04:04:12.168157Z",
     "start_time": "2024-02-16T04:04:12.122596Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Final_DataFrame_Combined['ZIP'] = Final_DataFrame_Combined.Address.apply(get_ZIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T04:04:27.321986Z",
     "start_time": "2024-02-16T04:04:27.240408Z"
    }
   },
   "outputs": [],
   "source": [
    "Final_DataFrame_Combined.Price.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section IV: Data analysis and visualization\n",
    "\n",
    "We break this section into six sub sections:\n",
    "- Part I: Restaurants ratings distribution analysis\n",
    "- Part II: Restaurants categorical analysis\n",
    "- Part III: Restaurants operation hours analysis\n",
    "- Part IV: Restaurants reviews distribution analysis based on cities\n",
    "- Part V: Restaurants geo-spatial analysis\n",
    "- Part VI: Other categorical variables analysis\n",
    "\n",
    "After this above workflow, we expect to gain strong data insights from graphical methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Restaurants ratings distribution analysis \n",
    "\n",
    "- 1. Overall ratings distributions.\n",
    "- 2. Individual cities ratings distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distribution_plot:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    def plot(self, font_size, title, text_height):\n",
    "        counts = self.df['Reviews'].value_counts().sort_index()\n",
    "        ax = sns.barplot(x=counts.index, y=counts.values, alpha=0.8)\n",
    "        plt.title(title, fontsize=font_size)\n",
    "        plt.xlabel(\"Ratings\", fontsize=font_size)\n",
    "        plt.ylabel(\"# of Restaurants\", fontsize=font_size)\n",
    "        for rect, count in zip(ax.patches, counts.values):\n",
    "            height = rect.get_height()\n",
    "            ax.text(rect.get_x() + rect.get_width() / 2, height + text_height, count,\n",
    "                    ha='center', va='bottom')         \n",
    "    def clean_each_city_df(self):\n",
    "        exceptions(self.df)\n",
    "        clean_df = self.df.applymap(find_null)\n",
    "        return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T04:05:54.713659Z",
     "start_time": "2024-02-16T04:05:54.303760Z"
    }
   },
   "outputs": [],
   "source": [
    "Final_DataFrame_Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T11:15:41.989304Z",
     "start_time": "2024-02-16T11:15:41.904112Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "ratdist = Distribution_plot(Final_DataFrame_Combined)\n",
    "ratdist.plot(font_size=10,title=\"Overall Rating Distribution\",text_height=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation and comments:**\n",
    "\n",
    "From the above plot:\n",
    "- 1.The overall restaurants ratings distribution was **left skewed**, which means majority of restaurants have ratings between 3.5 to 5.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(rc={'figure.figsize': (30, 15)})\n",
    "sns.set(font_scale=1.3)\n",
    "plt.subplots(3, 2, figsize=(30, 15))\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.4)\n",
    "city_data_dict = {\n",
    "    \"San Francisco\": San_Fransisco_data,\n",
    "    \"Fresno\": Fresno_data,\n",
    "    \"San Diego\": San_Diego_data,\n",
    "    \"Sacramento\": Sacramento_data,\n",
    "    \"Los Angeles\": LA_data,\n",
    "    \"San Jose\": San_Jose_data,\n",
    "}\n",
    "jumbled_cities_order = [\n",
    "    \"San Francisco\",\n",
    "    \"Fresno\",\n",
    "    \"San Diego\",\n",
    "    \"Sacramento\",\n",
    "    \"Los Angeles\",\n",
    "    \"San Jose\",\n",
    "]\n",
    "for index, city_name in enumerate(jumbled_cities_order):\n",
    "    plt.subplot(3, 2, index + 1)\n",
    "    current_city_data = city_data_dict[city_name]\n",
    "    ratdist = Distribution_plot(current_city_data)\n",
    "    clean_city_data = ratdist.clean_each_city_df()\n",
    "    ratdist.plot(20, f\"{city_name} Rating Distribution\", text_height=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(rc={'figure.figsize': (30, 15)})\n",
    "sns.set(font_scale=1.3)\n",
    "plt.subplots(3, 2, figsize=(30, 15))\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.7)\n",
    "city_plot_data = {\n",
    "    \"Long Beach\": Long_Beach_data,\n",
    "    \"Oakland\": Oakland_data,\n",
    "    \"Bakersfield\": Bakersfield_data,\n",
    "    \"Anaheim\": Anaheim_data,\n",
    "    \"Santa Ana\": Santa_Ana_data,\n",
    "    \"Riverside\": Riverside_data,\n",
    "}\n",
    "city_order = [\n",
    "    \"Anaheim\",\n",
    "    \"Riverside\",\n",
    "    \"Long Beach\",\n",
    "    \"Santa Ana\",\n",
    "    \"Oakland\",\n",
    "    \"Bakersfield\",\n",
    "]\n",
    "for index, city_name in enumerate(city_order):\n",
    "    plt.subplot(3, 2, index + 1)\n",
    "    current_city_data = city_plot_data[city_name]\n",
    "    ratdist = Distribution_plot(current_city_data)\n",
    "    clean_city_data = ratdist.clean_each_city_df()\n",
    "    ratdist.plot(20, f\"{city_name} Rating Distribution\", text_height=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation and comments:**\n",
    "\n",
    "From the above plot:\n",
    "- 1. Generally, Los Angeles, San Diego, San Francisco, Oakland have lowest portion of restaurants with rating below 4.0.\n",
    "- 2. Fresno, Bakersfield, and Riverside have more frequency of low ratings restaurants compared with other cities. \n",
    "- 3. Majority of ratings are 3.5 to 4.5, we saw the geneous of Yelp reviewers on these restaurants.\n",
    "- 4. Bakersfield and Riverside had relatively uniform rating distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Restaurants categorical analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T11:35:43.051504Z",
     "start_time": "2024-02-16T11:35:42.583275Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import sys\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import nltk\n",
    "# import string\n",
    "# import warnings\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from wordcloud import WordCloud \n",
    "# from nltk.corpus import stopwords\n",
    "# from Feature_Engineering import *\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# class CategoryFrequencyPlotter:\n",
    "#     def __init__(self, data, remove=string.punctuation):\n",
    "#         self.pattern = fr\"[{remove}]\"\n",
    "#         self.cat_frequency = {}\n",
    "#         self.data = data\n",
    "        \n",
    "#     def _regex_split_join(self, item):\n",
    "#         try:\n",
    "#             to_be_join = re.split(self.pattern, item.strip())\n",
    "#         except:\n",
    "#             raise Exception(f'The item --{item} in the line above is not a string')\n",
    "#         for word in to_be_join:\n",
    "#             new_word = word.strip()\n",
    "#             if new_word in self.cat_frequency:\n",
    "#                 self.cat_frequency[new_word] += 1\n",
    "#             else:\n",
    "#                 self.cat_frequency[new_word] = 1\n",
    "#         return \" \".join(to_be_join)\n",
    "    \n",
    "#     def category_counting(self):\n",
    "#         self.data[['Category']].applymap(self._regex_split_join)  \n",
    "#         del self.cat_frequency['Unknown'] \n",
    "#         if '' in self.cat_frequency:\n",
    "#             del self.cat_frequency['']  \n",
    "#         return self.cat_frequency\n",
    "    \n",
    "#     def cat_plot(self, num_top, font_size, title, overall=False, verbose=False):\n",
    "#         catfre_df = pd.DataFrame(self.cat_frequency.items(), columns=['Word_Categories', 'Frequency'])\n",
    "#         catfre_df = catfre_df.sort_values('Frequency', ascending=False)\n",
    "#         tot_cat = catfre_df.Word_Categories.value_counts()\n",
    "#         if verbose:\n",
    "#             print(f\"There are {len(tot_cat)} different word categories to describe restaurants in Yelp\")\n",
    "\n",
    "#         top_cat = catfre_df.Word_Categories.iloc[0:num_top]\n",
    "#         top_fre = catfre_df.Frequency.iloc[0:num_top]\n",
    "#         if overall:\n",
    "#             plt.figure(figsize=(20, 14))\n",
    "#         ax = sns.barplot(x=top_fre.values, y=top_cat.values, alpha=0.8)\n",
    "#         plt.title(title, fontsize=font_size)\n",
    "#         plt.ylabel('Word Categories', fontsize=font_size)\n",
    "#         plt.xlabel('Word Frequency', fontsize=font_size)\n",
    "\n",
    "#         for rect, label in zip(ax.patches, top_fre.values):\n",
    "#             height = rect.get_height()\n",
    "#             ax.text(rect.get_x() + rect.get_width() + 5, height / 2 + rect.get_y(), label, ha='left')\n",
    "            \n",
    "#     def make_word_cloud(self, text, max_word=30):\n",
    "#         wordcloud = WordCloud(\n",
    "#             background_color='white',\n",
    "#             stopwords=stopwords.words(\"english\"),\n",
    "#             scale=10,\n",
    "#             max_words=max_word,\n",
    "#             max_font_size=40)\n",
    "#         wordcloud = wordcloud.generate_from_frequencies(frequencies=self.cat_frequency)\n",
    "#         plt.figure(1, figsize=(15, 15))\n",
    "#         plt.axis('off')\n",
    "#         plt.imshow(wordcloud, interpolation=\"bilinear\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud \n",
    "from nltk.corpus import stopwords\n",
    "from Feature_Engineering import *\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class CategoryFrequencyPlotter:\n",
    "    def __init__(self, dataframe, punctuation_to_remove=string.punctuation):\n",
    "        self.regex_pattern = fr\"[{punctuation_to_remove}]\"\n",
    "        self.frequency_dict = {}\n",
    "        self.df = dataframe\n",
    "        \n",
    "    def _split_clean_join(self, text):\n",
    "        try:\n",
    "            split_text = re.split(self.regex_pattern, text.strip())\n",
    "        except:\n",
    "            raise Exception(f'Error: The text \"{text}\" is not a string')\n",
    "        for word in split_text:\n",
    "            cleaned_word = word.strip()\n",
    "            if cleaned_word in self.frequency_dict:\n",
    "                self.frequency_dict[cleaned_word] += 1\n",
    "            else:\n",
    "                self.frequency_dict[cleaned_word] = 0 if cleaned_word == '' else 1\n",
    "        return \" \".join(split_text)\n",
    "    \n",
    "    def count_categories(self):\n",
    "        self.df[['Category']].applymap(self._split_clean_join)  \n",
    "        self.frequency_dict.pop('Unknown', None) \n",
    "        self.frequency_dict.pop('', None)  \n",
    "        return self.frequency_dict\n",
    "    \n",
    "    def plot_category_frequencies(self, top_n, title_font_size, chart_title, include_overall=False, show_details=False):\n",
    "        cat_freq_dataframe = pd.DataFrame(list(self.frequency_dict.items()), columns=['Category', 'Count']).sort_values('Count', ascending=False)\n",
    "        total_categories = cat_freq_dataframe.Category.nunique()\n",
    "        if show_details:\n",
    "            print(f\"Total {total_categories} unique word categories found in Yelp restaurant descriptions\")\n",
    "\n",
    "        top_categories = cat_freq_dataframe.head(top_n)\n",
    "        if include_overall:\n",
    "            plt.figure(figsize=(20, 14))\n",
    "        sns.barplot(x='Count', y='Category', data=top_categories, alpha=0.8)\n",
    "        plt.title(chart_title, fontsize=title_font_size)\n",
    "        plt.xlabel('Frequency', fontsize=title_font_size)\n",
    "        plt.ylabel('Categories', fontsize=title_font_size)\n",
    "\n",
    "        ax = plt.gca()\n",
    "        for patch in ax.patches:\n",
    "            ax.text(patch.get_x() + patch.get_width() + 5, patch.get_y() + patch.get_height() / 2, patch.get_width(), ha='left')\n",
    "    \n",
    "    def generate_wordcloud(self, max_words=30):\n",
    "        wordcloud_gen = WordCloud(\n",
    "            background_color='white',\n",
    "            stopwords=set(stopwords.words(\"english\")),\n",
    "            scale=10,\n",
    "            max_words=max_words,\n",
    "            max_font_size=40\n",
    "        ).generate_from_frequencies(frequencies=self.frequency_dict)\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(wordcloud_gen, interpolation=\"bilinear\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T11:35:43.596060Z",
     "start_time": "2024-02-16T11:35:43.124993Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(25,25)})\n",
    "sns.set(font_scale=1.3)\n",
    "Frequecy_of_categories = CategoryFrequencyPlotter(Final_DataFrame_Combined)\n",
    "Frequecy_of_categories.count_categories()\n",
    "Frequecy_of_categories.plot_category_frequencies(30, 20, \"Bar plot of the top 30 word categories\", include_overall=True, show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale=1.3)\n",
    "\n",
    "obj = Distribution_plot(LA_data)\n",
    "LA_data_clean=obj.clean_each_city_df()\n",
    "\n",
    "obj = Distribution_plot(San_Diego_data)\n",
    "San_Diego_data_clean=obj.clean_each_city_df()\n",
    "\n",
    "obj = Distribution_plot(San_Jose_data)\n",
    "San_Jose_data_clean=obj.clean_each_city_df()\n",
    "\n",
    "obj = Distribution_plot(San_Fransisco_data)\n",
    "San_Fransisco_data_clean=obj.clean_each_city_df()\n",
    "\n",
    "obj = Distribution_plot(Fresno_data)\n",
    "Fresno_data_clean=obj.clean_each_city_df()\n",
    "\n",
    "obj = Distribution_plot(Sacramento_data)\n",
    "Sacramento_data_clean=obj.clean_each_city_df()\n",
    "\n",
    "cities_data = {\n",
    "    \"Los Angeles\": LA_data_clean,\n",
    "    \"San Diego\": San_Diego_data_clean,\n",
    "    \"San Jose\": San_Jose_data_clean,\n",
    "    \"San Francisco\": San_Fransisco_data_clean,\n",
    "    \"Fresno\": Fresno_data_clean,\n",
    "    \"Sacramento\": Sacramento_data_clean,\n",
    "}\n",
    "plt.figure(figsize=(20, 30)) \n",
    "for index, (city_name, city_data) in enumerate(cities_data.items(), start=1):\n",
    "    plt.subplot(3, 2, index)\n",
    "    Frequecy_of_categoriester = CategoryFrequencyPlotter(city_data)\n",
    "    Frequecy_of_categoriester.count_categories()\n",
    "    Frequecy_of_categoriester.plot_category_frequencies(10, 20, f\"{city_name} Top 10 word categories\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(font_scale=1.3)\n",
    "\n",
    "obj = Distribution_plot(Long_Beach_data)\n",
    "Long_Beach_data_clean=obj.clean_each_city_df()\n",
    "\n",
    "obj = Distribution_plot(Oakland_data)\n",
    "Oakland_data_clean=obj.clean_each_city_df()\n",
    "\n",
    "obj = Distribution_plot(Bakersfield_data)\n",
    "Bakersfield_data_clean=obj.clean_each_city_df()\n",
    "\n",
    "obj = Distribution_plot(Anaheim_data)\n",
    "Anaheim_data_clean=obj.clean_each_city_df()\n",
    "\n",
    "obj = Distribution_plot(Santa_Ana_data)\n",
    "Santa_Ana_data_clean=obj.clean_each_city_df()\n",
    "\n",
    "obj = Distribution_plot(Riverside_data)\n",
    "Riverside_data_clean=obj.clean_each_city_df()\n",
    "\n",
    "cities_data = {\n",
    "    \"Long Beach\": Long_Beach_data_clean,\n",
    "    \"Oakland\": Oakland_data_clean,\n",
    "    \"Bakersfield\": Bakersfield_data_clean,\n",
    "    \"Anaheim\": Anaheim_data_clean,\n",
    "    \"Santa Ana\": Santa_Ana_data_clean,\n",
    "    \"Riverside\": Riverside_data_clean,\n",
    "}\n",
    "plt.figure(figsize=(20, 30)) \n",
    "for index, (city_name, city_data) in enumerate(cities_data.items(), start=1):\n",
    "    plt.subplot(3, 2, index)\n",
    "    Frequecy_of_categoriester = CategoryFrequencyPlotter(city_data)\n",
    "    Frequecy_of_categoriester.count_categories()\n",
    "    Frequecy_of_categoriester.plot_category_frequencies(10, 20, f\"{city_name} Top 10 word categories\")\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_DataFrame_Combined[\"Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Frequecy_of_categories = CategoryFrequencyPlotter(Final_DataFrame_Combined)\n",
    "Frequecy_of_categories.count_categories()\n",
    "Frequecy_of_categories.generate_wordcloud(max_words=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Restaurants operation hours analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OpeningTime(time_str):\n",
    "    if time_str == None:\n",
    "        return None\n",
    "    if time_str == 'Closed':\n",
    "         return 'Closed'\n",
    "    print(\"Actual string\", time_str)\n",
    "    components = str(time_str[0][0]).split()\n",
    "    print(\"time_str:\",components)\n",
    "    print(\"Opening component:\",type(components[0]))\n",
    "    opening_hour_component = str(components[0]).split(':')\n",
    "    if len(opening_hour_component) != 2:\n",
    "        return \"Closed\"\n",
    "    opening_hour, opening_minute = map(int, opening_hour_component)\n",
    "    if 'PM' in components[1] and opening_hour != 12:\n",
    "        opening_hour += 12\n",
    "    opening_minute = opening_minute if opening_minute != 0 else '00'\n",
    "    return str(opening_hour)+\":\"+str(opening_minute)\n",
    "\n",
    "time_str = [['3:00 PM - 9:30 PM']]\n",
    "hour_24_format = OpeningTime(time_str)\n",
    "print(hour_24_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClosingTime(time_str):\n",
    "    if time_str == None or time_str==\"Unknown\":\n",
    "        return 'No Info'\n",
    "    if time_str == 'Closed':\n",
    "        return 'Closed'\n",
    "\n",
    "    try:\n",
    "        print(\"Actual string\", time_str)\n",
    "        components = str(time_str[0][1]).split()\n",
    "        print(\"time_str:\",components)\n",
    "        print(\"closing component:\",components[0])\n",
    "    except:\n",
    "        components = str(time_str).split(\" - \")[-1].split()[0:2]\n",
    "        print(time_str, \"################\", components)\n",
    "  \n",
    "    closing_hour_component = str(components[0]).split(':')\n",
    "    if len(closing_hour_component) != 2:\n",
    "        return \"Closed\"\n",
    "    closing_hour, closing_minute = map(int, closing_hour_component)\n",
    "    if 'PM' in components[1] and closing_hour != 12:\n",
    "        closing_hour += 12\n",
    "    closing_minute = closing_minute if closing_minute != 0 else '00'\n",
    "    return str(closing_hour)+\":\"+str(closing_minute)\n",
    "\n",
    "time_str = [['9:00 PM ','10:30 PM']]\n",
    "time_str = [['7:00 AM ','11:00 AM']]\n",
    "closing_hour_24 = ClosingTime(time_str)\n",
    "print(closing_hour_24) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_DataFrame_Combined[['MonOpen', 'TueOpen', 'WedOpen', 'ThuOpen', 'FriOpen', 'SatOpen', \n",
    "               'SunOpen']] = Final_DataFrame_Combined[['Mon', 'Tue', 'Wed', 'Thu', 'Fri', \n",
    "                                            'Sat', 'Sun']].applymap(OpeningTime)\n",
    "\n",
    "Final_DataFrame_Combined[['MonClose', 'TueClose', 'WedClose', 'ThuClose', 'FriClose', 'SatClose', \n",
    "               'SunClose']] = Final_DataFrame_Combined[['Mon', 'Tue', 'Wed', 'Thu', 'Fri', \n",
    "                                            'Sat', 'Sun']].applymap(ClosingTime)\n",
    "Final_DataFrame_Combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IV: Top 25 restaurants reviews distribution analysis based on cities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_DataFrame_Combined['Reviews'] = pd.to_numeric(Final_DataFrame_Combined['Reviews'], errors='coerce').fillna(0)\n",
    "Final_DataFrame_Combined.loc[Final_DataFrame_Combined['Reviews'] == 'Unknown', 'Reviews'] = 0\n",
    "Final_DataFrame_Combined['Reviews'] = Final_DataFrame_Combined['Reviews'].astype(float)\n",
    "Final_DataFrame_Combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ratings(rating_str):\n",
    "    try:\n",
    "        return int(''.join(re.findall(r'\\d+', rating_str)).replace(',', ''))\n",
    "    except (ValueError, AttributeError):\n",
    "        return 0\n",
    "\n",
    "Final_DataFrame_Combined['Rating'] = Final_DataFrame_Combined['Rating'].apply(extract_ratings)\n",
    "Final_DataFrame_Combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq_rev_df = Final_DataFrame_Combined.sort_values('Rating', ascending=False)\n",
    "v = Freq_rev_df['Name'].values.tolist()[0:25]\n",
    "d = Freq_rev_df['Rating'].values.tolist()[0:25]\n",
    "color_series = ['#FAE927','#E9E416','#C9DA36','#9ECB3C','#6DBC49',\n",
    "                '#37B44E','#3DBA78','#14ADCF','#209AC9','#1E91CA',\n",
    "                '#2C6BA0','#2B55A1','#2D3D8E','#44388E','#7D3990',\n",
    "                '#A63F98','#C31C88','#D52178','#D5225B','#D02C2A',\n",
    "                '#D44C2D','#F57A34','#FA8F2F','#D99D21','#CF7B25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq_rev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie1 = Pie(init_opts=opts.InitOpts(width='1000px', height='800px'))\n",
    "pie1.set_colors(color_series)\n",
    "pie1.add(\"\", [list(z) for z in zip(v, d)],\n",
    "        radius=[\"30%\", \"135%\"],\n",
    "        center=[\"50%\", \"65%\"],\n",
    "        rosetype=\"area\")\n",
    "pie1.set_global_opts(title_opts=opts.TitleOpts(title='Top 25 most popular restaurants with most reviews'),\n",
    "                     legend_opts=opts.LegendOpts(is_show=False),\n",
    "                     toolbox_opts=opts.ToolboxOpts())\n",
    "pie1.set_series_opts(label_opts=opts.LabelOpts(is_show=True, position=\"inside\", font_size=12,\n",
    "                                               formatter=\"{b}:{c}\", font_style=\"italic\",\n",
    "                                               font_weight=\"bold\"))\n",
    "pie1.render_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations and Comments:**\n",
    "\n",
    "- The plot above showed top 25 most reviewed restaurants by this Archimedean spiral-liked plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like perfrom some further analysis combined graphical methods on these top 25 restaurants, and we before with a list of these ideal restaurants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_rets = ['Bottega Louie',\n",
    " \"Brenda's French Soul Food\",\n",
    " \"Porto's Bakery & Cafe\",\n",
    " 'Daikokuya Little Tokyo',\n",
    " 'Wurstk√ºche',\n",
    " 'Tartine Bakery & Cafe',\n",
    " 'Perch',\n",
    " 'Lucha Libre Gourmet Taco Shop',\n",
    " 'Hash House A Go Go',\n",
    " 'House Of Prime Rib',\n",
    " \"Pink's Hot Dogs\",\n",
    " 'San Tung',\n",
    " 'Snooze, An A.M. Eatery',\n",
    " 'Burma Superstar',\n",
    " \"Howlin' Ray's\",\n",
    " 'Philippe the Original',\n",
    " 'Bestia',\n",
    " 'Fog Harbor Fish House',\n",
    " 'Hog Island Oyster Co',\n",
    " 'Eggslut',\n",
    " 'The Attic',\n",
    " 'The Slanted Door',\n",
    " 'Homeroom',\n",
    " 'Gary Danko',\n",
    " 'Orenchi Ramen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ideal_rests_filter(item, ideal_rets=ideal_rets):\n",
    "    if item.strip() in ideal_rets:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Freq_rev_df['Price_for_plot'] = Freq_rev_df['Price'].replace({'\\$\\$\\$': 'USD 35-65', '\\$\\$': 'USD 16-35', '\\$': 'USD 0-15'}, regex=True)\n",
    "sns.set(rc={'figure.figsize':(25, 25)})\n",
    "sns.set(font_scale=1.3)\n",
    "cat_plot = sns.catplot(x='Reviews', hue='Price_for_plot', kind='count', data=Freq_rev_df[:25], height=5, aspect=2)\n",
    "plt.ylabel('Restaurants Counts', fontsize=20)\n",
    "plt.xlabel('Restaurants ratings', fontsize=20)\n",
    "plt.title('How Price affects Rating Points in Popular Restaurants', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part V: Restaurants geo-spatial analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_DataFrame_Combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_DataFrame_Combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intRating(item):\n",
    "    if item == 'Unknown':\n",
    "        return 0\n",
    "    return float(item)\n",
    "Final_DataFrame_Combined['ReviewRatings'] = Final_DataFrame_Combined['Reviews'].apply(intRating)\n",
    "Final_DataFrame_Combined['ZIP'] = Final_DataFrame_Combined['ZIP'].replace({'CA ': ''}, regex=True)\n",
    "Postal_code_data = Final_DataFrame_Combined.groupby('ZIP', as_index=False)['ReviewRatings'].mean()\n",
    "Postal_code_data = Postal_code_data[Postal_code_data['ZIP'] != 'Unknown'].astype(str)\n",
    "Postal_code_data['ReviewRatings'] = Postal_code_data['ReviewRatings']\n",
    "Postal_code_data.reset_index(drop=True, inplace=True)\n",
    "Postal_code_data['ZIP'] = Postal_code_data['ZIP'].str.strip()\n",
    "print(Postal_code_data.head())\n",
    "print(Postal_code_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Locations_data = pd.read_csv('../Stored_Info/City_data/Extra_info.csv', dtype={'ZIP':str,'STCOUNTYFP':str})\n",
    "Locations_data['ZIP'] = Locations_data['ZIP'].str.strip()\n",
    "print(Locations_data[Locations_data[\"ZIP\"]==\"90004\"].head())\n",
    "scratch = Postal_code_data.merge(Locations_data, how='left', on='ZIP')\n",
    "scratch = scratch.dropna()\n",
    "scratch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    counties = json.load(response)\n",
    "countyFeatures = counties['features']\n",
    "needCountieFeatures= []\n",
    "for sample in countyFeatures:\n",
    "    if sample['properties']['STATE'] == '06':\n",
    "        needCountieFeatures.append(sample)\n",
    "plot_counties = {}\n",
    "plot_counties['type'] = counties['type'] \n",
    "plot_counties['features'] = needCountieFeatures\n",
    "print(plot_counties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch.head()\n",
    "print(scratch.dtypes)\n",
    "scratch['ReviewRatings'] = scratch['ReviewRatings'].astype(float).round(2)\n",
    "print(scratch.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(\n",
    "    scratch,\n",
    "    geojson=plot_counties,  \n",
    "    locations='STCOUNTYFP',\n",
    "    color='ReviewRatings',\n",
    "    color_continuous_scale=\"Viridis\",\n",
    "    range_color=(0, 5), \n",
    "    scope=\"usa\"\n",
    ")\n",
    "fig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section V: Future works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
